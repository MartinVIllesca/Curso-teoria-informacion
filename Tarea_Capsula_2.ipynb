{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea CÃ¡psula #2: *Representation Learning*\n",
    "\n",
    "### EL7024 - TeorÃ­a de la InformaciÃ³n: Fundamentos y Aplicaciones \n",
    "### Information and Decision Systems Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas TeÃ³ricas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1) Inferencia Variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) A partir del funcional objetivo de Inferencia Variacional $KL(q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x}))$ encuentre una cota inferior para el logaritmo de la **evidencia** $log p(x)$ y muestre que esta se puede escribir como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "ELBO(q) &= \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{p(\\mathbf{x},\\mathbf{z})}\\} - \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{q(\\mathbf{z})}\\} \\\\\n",
    "&= \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{p(\\mathbf{x}|\\mathbf{z})}\\} - KL(q(\\mathbf{z})||p(\\mathbf{z}))\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Explique e interprete la forma de la *Evidence Lower Bound*. En este sentido puede resultarle mÃ¡s fÃ¡cil considerar la segunda expresiÃ³n $ELBO(q) = \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{p(\\mathbf{x}|\\mathbf{z})}\\} - KL(q(\\mathbf{z})||p(\\mathbf{z}))$.\n",
    "\n",
    "**Hint**: Puede resultarle Ãºtil pensar en lo siguiente: Â¿Para quÃ© valores de $\\mathbf{z}$ o bajo quÃ© criterio cada tÃ©rmino de este funcional propicia la asignaciÃ³n de masa en $q(\\mathbf{z})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Explique brevemente por quÃ© los problemas de optimizaciÃ³n $\\underset{q(\\mathbf{z}) \\in \\mathcal{Q}}{\\operatorname{arg min}} KL(q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x}))$ y $\\underset{q(\\mathbf{z}) \\in \\mathcal{Q}}{\\operatorname{arg max}} ELBO(q)$ son equivalentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2) Information Bottleneck Profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el mismo estilo que estudiamos para los *Autoencoders* Variacionales deduciremos la forma de definir una funciÃ³n de pÃ©rdida adecuada para abordar el problema del *Information Bottleneck* por medio de redes neuronales. Para esto, primero reescribimos el problema de la siguiente manera:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{p(\\mathbf{z}|\\mathbf{x}), p(\\mathbf{y}|\\mathbf{z})}{\\operatorname{max}} R_{IB} = I(\\mathbf{Y};\\mathbf{Z}) - \\beta I(\\mathbf{X};\\mathbf{Z})\n",
    "\\end{equation}\n",
    "\n",
    "de modo tal que deduciremos una cota inferior del funcional, siendo $\\mathbf{Z}$ la variable latente que comprime la seÃ±al de entrada $\\mathbf{X}$ y se utiliza en la predicciÃ³n de $\\mathbf{Y}$, y es tal que $\\mathbf{Z} - \\mathbf{X} - \\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Consideremos primero una cota inferior para $I(\\mathbf{Y};\\mathbf{Z})$. Considerando que $q(\\mathbf{y}|\\mathbf{z})$ es una aproximaciÃ³n variacional del *decoder* $p(\\mathbf{y}|\\mathbf{z})$ y el supuesto de Markovianidad demuestre que:\n",
    "\n",
    "\\begin{equation}\n",
    "    I(\\mathbf{Y};\\mathbf{Z}) \\geq \\int_{\\mathcal{X}}\\int_{\\mathcal{Z}}\\int_{\\mathcal{Y}}p(z|x)p(y|x)p(x)\\log q(y|z) dydzdx = \\tilde{I}(\\mathbf{Y};\\mathbf{Z})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Para el tÃ©rmino $I(\\mathbf{X};\\mathbf{Z})$ es necesaria una cota superior. Demuestre que una aproximaciÃ³n variacional $r(\\mathbf{z})$ del *prior* $p(\\mathbf{z})$ satisface que:\n",
    "\n",
    "\\begin{equation}\n",
    "    I(\\mathbf{X};\\mathbf{Z}) \\leq \\int_{\\mathcal{X}}\\int_{\\mathcal{Z}}\\int_{\\mathcal{Y}}p(z|x)p(y|x)p(x)\\log \\frac{p(z|x)}{r(z)} dydzdx = \\tilde{I}(\\mathbf{X};\\mathbf{Z})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos la cota resultante para $R_{IB}$ por\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{IB} = \\tilde{I}(\\mathbf{Y};\\mathbf{Z}) - \\beta \\tilde{I}(\\mathbf{X};\\mathbf{Z})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Empleando la distribuciÃ³n conjunta empÃ­rica $\\tilde{p}(x,y) = \\frac{1}{N}\\sum_{i=1}^{N}\\delta_{x^{(i)}}(x)\\delta_{y^{(i)}}(y)$ deduzca que\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{IB} \\approx \\frac{1}{N}\\sum_{i=1}^{N}\\left[ \\mathbb{E}_{\\mathbf{Z} \\sim p(\\mathbf{z}|\\mathbf{x})} \\left\\{ \\log q(\\mathbf{y}^{(i)}|\\mathbf{z}) \\right\\} - \\beta KL\\left( p(\\mathbf{z}|\\mathbf{x}^{(i)}) || r(\\mathbf{z}) \\right)\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas de SimulaciÃ³n:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte de la tarea deberÃ¡n implementar y probar el modelo deducido en la pregunta 2 mediante redes neuronales, donde se considera que las aproximaciones variacionales estÃ¡n parametrizadas por una red correspondiente al *encoder* $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ y otra para el *decoder* $q_{\\phi}(\\mathbf{y}|\\mathbf{z})$, es decir que el funcional a **minimizar** corresponde a:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\mathcal{L}'_{IB}(\\phi, \\theta) &= -\\frac{1}{N}\\sum_{i=1}^{N}\\left[ \\mathbb{E}_{\\mathbf{Z} \\sim p_{\\theta}(\\mathbf{z}|\\mathbf{x})} \\left\\{ \\log q_{\\phi}(\\mathbf{y}^{(i)}|\\mathbf{z}) \\right\\} - \\beta KL\\left( p_\\theta(\\mathbf{z}|\\mathbf{x}^{(i)}) || r(\\mathbf{z}) \\right)\\right] \\\\\n",
    "        &= \\frac{1}{N}\\sum_{i=1}^{N}\\left[ \\mathbb{E}_{\\mathbf{Z} \\sim p_{\\theta}(\\mathbf{z}|\\mathbf{x})} \\left\\{ -\\log q_{\\phi}(\\mathbf{y}^{(i)}|\\mathbf{z}) \\right\\} + \\beta KL\\left( p_\\theta(\\mathbf{z}|\\mathbf{x}^{(i)}) || r(\\mathbf{z}) \\right)\\right]\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Los experimentos se realizarÃ¡n sobre el *dataset* MNIST en el problema de **clasificaciÃ³n**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Implemente una clase $\\texttt{DeepIB}$ que corresponda a la siguiente arquitectura:\n",
    "\n",
    "* *Encoder* Gaussiano: Red *fully-connected* $784\\to1024\\to1024\\to2K$, donde K es la dimensionalidad de la variable latente. Los primeros $K$ valores de dicha capa corresponderÃ¡n a $\\mu_{\\mathbf{z}}(\\mathbf{x})$ y las siguientes $K$ componentes comprenden las desviaciones estÃ¡ndar $\\sigma_{\\mathbf{z}}(\\mathbf{x})$. Entre cada capa debe usar la funciÃ³n de activaciÃ³n $\\texttt{ReLU}$.\n",
    "**Hint**: Para asegurar que las desviaciones estÃ¡ndar $\\sigma_{\\mathbf{z}}(\\mathbf{x})$ sean positivas se recomienda aplicarles la siguiente transformaciÃ³n: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma_{\\mathbf{z}}(\\mathbf{x}) = \\log (1 + \\exp (\\sigma_{\\mathbf{z}}^{raw}(\\mathbf{x}) - 5.0)),\n",
    "\\end{equation}\n",
    "\n",
    "$\\hspace{7mm}$ donde $\\sigma_{\\mathbf{z}}^{raw}(\\mathbf{x})$ denota a la desviaciÃ³n estÃ¡ndar antes de aplicar esta transformaciÃ³n.\n",
    "\n",
    "* *Decoder*: Red *fully-connected* $K\\to10$. Considere que este decoder es determinÃ­stico.\n",
    "\n",
    "Para la FunciÃ³n de Costo considere $r(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};0,I)$ (es decir que requiere una expresiÃ³n cerrada para el tÃ©rmino de divergencia).\n",
    "\n",
    "**Hint**: Dado que el *decoder* es determinÃ­stico, puede usar la funciÃ³n de *Cross-Entropy* para el primer tÃ©rmino, en forma astuta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Implemente una clase $\\texttt{MLP}$ correspondiente a la arquitectura $784\\to1024\\to1024\\to K\\to10$ para un mismo valor de $K$ que en el caso de $\\texttt{DeepIB}$. Al ser un problema de clasificaciÃ³n puede entrenar con *Cross-Entropy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Para $K=2$ y $\\beta \\in \\{10^{-3}, 10^{-1},1\\}$, entrene ambos modelos considerando un *loop* de entrenamiento similar al de la CÃ¡psula 2, de tal modo que el *learning rate* del optimizador Adam decaiga en $97\\%$ cada $2$ Ã©pocas:\n",
    "\n",
    "$\\texttt{lr0 = 10e-4}$\n",
    "\n",
    "$\\texttt{beta1 = 0.5}$\n",
    "\n",
    "$\\texttt{beta2 = 0.999}$\n",
    "\n",
    "$\\texttt{lr_decay = 0.97}$\n",
    "\n",
    "$\\texttt{decay_rate = 2}$\n",
    "\n",
    "$\\texttt{nEpochs = 200}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como parÃ¡metro $L$ de muestreo para aplicar el *Reparametrization Trick* puede utilizar $L=12$. Si usa la funciÃ³n de Pytorch [$\\texttt{torch.nn.CrossEntropyLoss}$](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) se recomienda revisar el efecto de usar $\\texttt{reduction='none'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare y analice la *accuracy* obtenida por los modelos estocÃ¡sticos obtenidos entre sÃ­ y con el modelo $\\texttt{MLP}$ clÃ¡sico.\n",
    "\n",
    "AdemÃ¡s, para los modelos estocÃ¡sticos, obtenga la proyecciÃ³n de los datos de prueba en el espacio latente $K$-dimensional y analice el efecto de $\\beta$ en este resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 1.x\n",
    "#%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from copy import deepcopy\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits import mplot3d\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4d94d08a4d4a079ce04c0a54bf4fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32caa7e546424b5092827bf7c2ef0102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f9f28748b04b71aadab7a4a7226bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b21e090ded94e37be18d6cdb51b66e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "## Batch Sizes\n",
    "train_batch_size = 100\n",
    "test_batch_size = 1000\n",
    "\n",
    "## Train-Val Split\n",
    "train_dataset_mnist = datasets.MNIST(root='./', train=True, download=True,\n",
    "                                     transform=transforms.Compose([\n",
    "                                         transforms.ToTensor()]))\n",
    "\n",
    "test_dataset_mnist = datasets.MNIST(root='./', train=False, \n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.ToTensor()]))\n",
    "\n",
    "## DataLoaders\n",
    "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist,\n",
    "                                                 batch_size = train_batch_size)\n",
    "test_loader_mnist = torch.utils.data.DataLoader(test_dataset_mnist,\n",
    "                                                batch_size = test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr0 = 10e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "lr_decay = 0.97\n",
    "decay_rate = 2\n",
    "nEpochs = 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}