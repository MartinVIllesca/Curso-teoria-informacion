{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Tarea_Capsula_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinVIllesca/Curso-teoria-informacion/blob/master/Tarea_Capsula_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvs96ka7Q0z",
        "colab_type": "text"
      },
      "source": [
        "# Tarea Cápsula #2: *Representation Learning*\n",
        "\n",
        "### EL7024 - Teoría de la Información: Fundamentos y Aplicaciones \n",
        "### Information and Decision Systems Group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHLXeBud7Q03",
        "colab_type": "text"
      },
      "source": [
        "Nombre: Martín Valderrama Illesca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi7iblhW7Q07",
        "colab_type": "text"
      },
      "source": [
        "## Preguntas Teóricas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwXXm2SS7Q0-",
        "colab_type": "text"
      },
      "source": [
        "### P1) Inferencia Variacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDb-dSYt7Q1A",
        "colab_type": "text"
      },
      "source": [
        "a) A partir del funcional objetivo de Inferencia Variacional $KL(q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x}))$ encuentre una cota inferior para el logaritmo de la **evidencia** $log p(x)$ y muestre que esta se puede escribir como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "ELBO(q) &= \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{p(\\mathbf{x},\\mathbf{z})}\\} - \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{q(\\mathbf{z})}\\} \\\\\n",
        "&= \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{p(\\mathbf{x}|\\mathbf{z})}\\} - KL(q(\\mathbf{z})||p(\\mathbf{z}))\n",
        "\\end{split}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9jwBqlu7Q1C",
        "colab_type": "text"
      },
      "source": [
        "b) Explique e interprete la forma de la *Evidence Lower Bound*. En este sentido puede resultarle más fácil considerar la segunda expresión $ELBO(q) = \\mathbb{E}_{\\mathbf{Z} \\sim q(\\mathbf{z})}\\{\\log{p(\\mathbf{x}|\\mathbf{z})}\\} - KL(q(\\mathbf{z})||p(\\mathbf{z}))$.\n",
        "\n",
        "**Hint**: Puede resultarle útil pensar en lo siguiente: ¿Para qué valores de $\\mathbf{z}$ o bajo qué criterio cada término de este funcional propicia la asignación de masa en $q(\\mathbf{z})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdSfNazB7Q1F",
        "colab_type": "text"
      },
      "source": [
        "c) Explique brevemente por qué los problemas de optimización $\\underset{q(\\mathbf{z}) \\in \\mathcal{Q}}{\\operatorname{arg min}} KL(q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x}))$ y $\\underset{q(\\mathbf{z}) \\in \\mathcal{Q}}{\\operatorname{arg max}} ELBO(q)$ son equivalentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML0kgbM17Q1H",
        "colab_type": "text"
      },
      "source": [
        "### P2) Information Bottleneck Profundo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN3EVwCH7Q1K",
        "colab_type": "text"
      },
      "source": [
        "En el mismo estilo que estudiamos para los *Autoencoders* Variacionales deduciremos la forma de definir una función de pérdida adecuada para abordar el problema del *Information Bottleneck* por medio de redes neuronales. Para esto, primero reescribimos el problema de la siguiente manera:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\underset{p(\\mathbf{z}|\\mathbf{x}), p(\\mathbf{y}|\\mathbf{z})}{\\operatorname{max}} R_{IB} = I(\\mathbf{Y};\\mathbf{Z}) - \\beta I(\\mathbf{X};\\mathbf{Z})\n",
        "\\end{equation}\n",
        "\n",
        "de modo tal que deduciremos una cota inferior del funcional, siendo $\\mathbf{Z}$ la variable latente que comprime la señal de entrada $\\mathbf{X}$ y se utiliza en la predicción de $\\mathbf{Y}$, y es tal que $\\mathbf{Z} - \\mathbf{X} - \\mathbf{Y}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3a7rKCI7Q1O",
        "colab_type": "text"
      },
      "source": [
        "a) Consideremos primero una cota inferior para $I(\\mathbf{Y};\\mathbf{Z})$. Considerando que $q(\\mathbf{y}|\\mathbf{z})$ es una aproximación variacional del *decoder* $p(\\mathbf{y}|\\mathbf{z})$ y el supuesto de Markovianidad demuestre que:\n",
        "\n",
        "\\begin{equation}\n",
        "    I(\\mathbf{Y};\\mathbf{Z}) \\geq \\int_{\\mathcal{X}}\\int_{\\mathcal{Z}}\\int_{\\mathcal{Y}}p(z|x)p(y|x)p(x)\\log q(y|z) dydzdx = \\tilde{I}(\\mathbf{Y};\\mathbf{Z})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_HE4ypl7Q1Q",
        "colab_type": "text"
      },
      "source": [
        "b) Para el término $I(\\mathbf{X};\\mathbf{Z})$ es necesaria una cota superior. Demuestre que una aproximación variacional $r(\\mathbf{z})$ del *prior* $p(\\mathbf{z})$ satisface que:\n",
        "\n",
        "\\begin{equation}\n",
        "    I(\\mathbf{X};\\mathbf{Z}) \\leq \\int_{\\mathcal{X}}\\int_{\\mathcal{Z}}\\int_{\\mathcal{Y}}p(z|x)p(y|x)p(x)\\log \\frac{p(z|x)}{r(z)} dydzdx = \\tilde{I}(\\mathbf{X};\\mathbf{Z})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZFeoLNv7Q1R",
        "colab_type": "text"
      },
      "source": [
        "Definamos la cota resultante para $R_{IB}$ por\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}_{IB} = \\tilde{I}(\\mathbf{Y};\\mathbf{Z}) - \\beta \\tilde{I}(\\mathbf{X};\\mathbf{Z})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKrswFxk7Q1U",
        "colab_type": "text"
      },
      "source": [
        "c) Empleando la distribución conjunta empírica $\\tilde{p}(x,y) = \\frac{1}{N}\\sum_{i=1}^{N}\\delta_{x^{(i)}}(x)\\delta_{y^{(i)}}(y)$ deduzca que\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}_{IB} \\approx \\frac{1}{N}\\sum_{i=1}^{N}\\left[ \\mathbb{E}_{\\mathbf{Z} \\sim p(\\mathbf{z}|\\mathbf{x})} \\left\\{ \\log q(\\mathbf{y}^{(i)}|\\mathbf{z}) \\right\\} - \\beta KL\\left( p(\\mathbf{z}|\\mathbf{x}^{(i)}) || r(\\mathbf{z}) \\right)\\right]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iNr1Epi7Q1X",
        "colab_type": "text"
      },
      "source": [
        "## Preguntas de Simulación:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2dhwpYx7Q1Z",
        "colab_type": "text"
      },
      "source": [
        "En esta parte de la tarea deberán implementar y probar el modelo deducido en la pregunta 2 mediante redes neuronales, donde se considera que las aproximaciones variacionales están parametrizadas por una red correspondiente al *encoder* $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ y otra para el *decoder* $q_{\\phi}(\\mathbf{y}|\\mathbf{z})$, es decir que el funcional a **minimizar** corresponde a:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{split}\n",
        "        \\mathcal{L}'_{IB}(\\phi, \\theta) &= -\\frac{1}{N}\\sum_{i=1}^{N}\\left[ \\mathbb{E}_{\\mathbf{Z} \\sim p_{\\theta}(\\mathbf{z}|\\mathbf{x})} \\left\\{ \\log q_{\\phi}(\\mathbf{y}^{(i)}|\\mathbf{z}) \\right\\} - \\beta KL\\left( p_\\theta(\\mathbf{z}|\\mathbf{x}^{(i)}) || r(\\mathbf{z}) \\right)\\right] \\\\\n",
        "        &= \\frac{1}{N}\\sum_{i=1}^{N}\\left[ \\mathbb{E}_{\\mathbf{Z} \\sim p_{\\theta}(\\mathbf{z}|\\mathbf{x})} \\left\\{ -\\log q_{\\phi}(\\mathbf{y}^{(i)}|\\mathbf{z}) \\right\\} + \\beta KL\\left( p_\\theta(\\mathbf{z}|\\mathbf{x}^{(i)}) || r(\\mathbf{z}) \\right)\\right]\n",
        "    \\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Los experimentos se realizarán sobre el *dataset* MNIST en el problema de **clasificación**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmeHdj5s7Q1d",
        "colab_type": "text"
      },
      "source": [
        "i) Implemente una clase $\\texttt{DeepIB}$ que corresponda a la siguiente arquitectura:\n",
        "\n",
        "* *Encoder* Gaussiano: Red *fully-connected* $784\\to1024\\to1024\\to2K$, donde K es la dimensionalidad de la variable latente. Los primeros $K$ valores de dicha capa corresponderán a $\\mu_{\\mathbf{z}}(\\mathbf{x})$ y las siguientes $K$ componentes comprenden las desviaciones estándar $\\sigma_{\\mathbf{z}}(\\mathbf{x})$. Entre cada capa debe usar la función de activación $\\texttt{ReLU}$.\n",
        "**Hint**: Para asegurar que las desviaciones estándar $\\sigma_{\\mathbf{z}}(\\mathbf{x})$ sean positivas se recomienda aplicarles la siguiente transformación: \n",
        "\n",
        "\\begin{equation}\n",
        "    \\sigma_{\\mathbf{z}}(\\mathbf{x}) = \\log (1 + \\exp (\\sigma_{\\mathbf{z}}^{raw}(\\mathbf{x}) - 5.0)),\n",
        "\\end{equation}\n",
        "\n",
        "$\\hspace{7mm}$ donde $\\sigma_{\\mathbf{z}}^{raw}(\\mathbf{x})$ denota a la desviación estándar antes de aplicar esta transformación.\n",
        "\n",
        "* *Decoder*: Red *fully-connected* $K\\to10$. Considere que este decoder es determinístico.\n",
        "\n",
        "Para la Función de Costo considere $r(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};0,I)$ (es decir que requiere una expresión cerrada para el término de divergencia).\n",
        "\n",
        "**Hint**: Dado que el *decoder* es determinístico, puede usar la función de *Cross-Entropy* para el primer término, en forma astuta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTt5S8A97Q1h",
        "colab_type": "text"
      },
      "source": [
        "ii) Implemente una clase $\\texttt{MLP}$ correspondiente a la arquitectura $784\\to1024\\to1024\\to K\\to10$ para un mismo valor de $K$ que en el caso de $\\texttt{DeepIB}$. Al ser un problema de clasificación puede entrenar con *Cross-Entropy*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC-h8bO67Q1k",
        "colab_type": "text"
      },
      "source": [
        "iii) Para $K=2$ y $\\beta \\in \\{10^{-3}, 10^{-1},1\\}$, entrene ambos modelos considerando un *loop* de entrenamiento similar al de la Cápsula 2, de tal modo que el *learning rate* del optimizador Adam decaiga en $97\\%$ cada $2$ épocas:\n",
        "\n",
        "$\\texttt{lr0 = 10e-4}$\n",
        "\n",
        "$\\texttt{beta1 = 0.5}$\n",
        "\n",
        "$\\texttt{beta2 = 0.999}$\n",
        "\n",
        "$\\texttt{lr_decay = 0.97}$\n",
        "\n",
        "$\\texttt{decay_rate = 2}$\n",
        "\n",
        "$\\texttt{nEpochs = 200}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijBDAN7u7Q1p",
        "colab_type": "text"
      },
      "source": [
        "Como parámetro $L$ de muestreo para aplicar el *Reparametrization Trick* puede utilizar $L=12$. Si usa la función de Pytorch [$\\texttt{torch.nn.CrossEntropyLoss}$](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) se recomienda revisar el efecto de usar $\\texttt{reduction='none'}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTKVhNZa7Q1r",
        "colab_type": "text"
      },
      "source": [
        "Compare y analice la *accuracy* obtenida por los modelos estocásticos obtenidos entre sí y con el modelo $\\texttt{MLP}$ clásico.\n",
        "\n",
        "Además, para los modelos estocásticos, obtenga la proyección de los datos de prueba en el espacio latente $K$-dimensional y analice el efecto de $\\beta$ en este resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0URp_cl7Q1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b00e4aea-13ca-4f69-8e28-91e3a2ee54b1"
      },
      "source": [
        "#%tensorflow_version 1.x\n",
        "#%load_ext tensorboard\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.distributions as D\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from copy import deepcopy\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits import mplot3d\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYm0rdZ77Q1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Batch Sizes\n",
        "train_batch_size = 100\n",
        "test_batch_size = 1000\n",
        "\n",
        "## Train-Val Split\n",
        "train_dataset_mnist = datasets.MNIST(root='./', train=True, download=True,\n",
        "                                     transform=transforms.Compose([\n",
        "                                         transforms.ToTensor()]))\n",
        "\n",
        "test_dataset_mnist = datasets.MNIST(root='./', train=False, \n",
        "                                    transform=transforms.Compose([\n",
        "                                        transforms.ToTensor()]))\n",
        "\n",
        "## DataLoaders\n",
        "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist,\n",
        "                                                 batch_size = train_batch_size)\n",
        "test_loader_mnist = torch.utils.data.DataLoader(test_dataset_mnist,\n",
        "                                                batch_size = test_batch_size)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1krnkYKE7Q2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr0 = 10e-4\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "lr_decay = 0.97\n",
        "decay_rate = 2\n",
        "nEpochs = 20\n",
        "\n",
        "zDim = 2\n",
        "L = 12"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9LbbDEgZgz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepIB(nn.Module):\n",
        "  '''Variational Autoencoder model with Gaussian Encoder and Decoder'''\n",
        "  def __init__(self, zDim, sampling = 1):\n",
        "    super(DeepIB, self).__init__()\n",
        "    self.sampling = sampling\n",
        "    hDim = 1024\n",
        "\n",
        "    ## Encoder\n",
        "    self.fc1 = nn.Sequential(nn.Linear(784, hDim), nn.ReLU())             ## Nonlinear layer\n",
        "    self.fc2 = nn.Sequential(nn.Linear(hDim, hDim), nn.ReLU())             ## Nonlinear layer\n",
        "    self.encoder_mu = nn.Linear(hDim, zDim)         ## Encoder for mu\n",
        "    self.encoder_sigma = nn.Linear(hDim, zDim)     ## Encoder for log-variance                              \n",
        "\n",
        "    ## Decoder\n",
        "    self.fc3 = nn.Sequential(nn.Linear(zDim, 10), nn.ReLU())         ## Nonlinear layer\n",
        "\n",
        "\n",
        "  def encode(self, x):\n",
        "    '''Make inferece of the latent variables Z for X'''\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    mu = self.encoder_mu(x)\n",
        "    sigma = self.encoder_sigma(x)-5\n",
        "    sigma = F.softplus(sigma - 5)\n",
        "    return mu, sigma\n",
        "\n",
        "  def reparametrize(self, mu, sigma):\n",
        "    '''Reparametrization is required for aproximating expectations during\n",
        "    training'''\n",
        "    L = self.sampling\n",
        "    sigma = torch.exp(0.5 * sigma)\n",
        "    if not self.training:\n",
        "      L = 1\n",
        "    epsilon = torch.randn(list(mu.shape)+[L]).cuda()\n",
        "    mu = mu.view(list(mu.shape)+[1])\n",
        "    sigma = sigma.view(list(sigma.shape)+[1])\n",
        "    z = mu + sigma * epsilon \n",
        "    z_ = z.permute(0,2,1)\n",
        "    return z_\n",
        "\n",
        "  def decode(self, z):\n",
        "    '''Reconstruct X from the latent space variables'''\n",
        "    z = self.fc3(z)\n",
        "    return z\n",
        "\n",
        "  def forward(self, x):\n",
        "    mu_z, logvar_z = self.encode(x)\n",
        "    z = self.reparametrize(mu_z, logvar_z)\n",
        "    y = torch.mean(self.decode(z), dim=1)\n",
        "    return mu_z, logvar_z, y\n",
        "\n",
        "def fVAELoss(y_true, y_pred, mu_z, logvar_z, beta):\n",
        "  '''Caculate the loss for VAE with Gaussian Encoder and Decoder'''\n",
        "  var_z = torch.exp(0.5 * logvar_z)\n",
        "  Jterm = 1 + torch.log(var_z) - (mu_z ** 2) - var_z\n",
        "  Jterm = 0.5 * Jterm.sum(axis = 1).reshape(-1)\n",
        "\n",
        "  # classification loss\n",
        "  Lterm = nn.CrossEntropyLoss()(y_pred, y_true.cuda())\n",
        "\n",
        "  loss = - Lterm + beta*Jterm.sum()\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfdHPIdDNy-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,K = 2):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(784, 1024),nn.ReLU())\n",
        "        self.fc2 = nn.Sequential(nn.Linear(1024, 1024),nn.ReLU())\n",
        "        self.fc3 = nn.Sequential(nn.Linear(1024, K),nn.ReLU())\n",
        "        self.fc4 = nn.Linear(K,10)\n",
        "    def forward(self,x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        out = self.fc4(x)\n",
        "        return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceLRK1GwZk2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "a7384c8f-9159-4698-9569-180bfdc995b0"
      },
      "source": [
        "model = DeepIB(zDim = zDim, sampling = L).cuda()\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr0, (beta1, beta2))\n",
        "for epoch in range(nEpochs+1):\n",
        "  for i, (x_b, y_b) in enumerate(train_loader_mnist):\n",
        "    x_b = x_b.reshape(train_batch_size, -1).cuda()\n",
        "\n",
        "    optimizer.zero_grad()           ## No olvidar resetear los gradientes\n",
        "    mu_z, logvar_z, y_pred = model.forward(x_b)\n",
        "    loss = -1 *  fVAELoss(y_b, y_pred, mu_z, logvar_z, beta=0.0001)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0 or epoch < 10:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_test = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader_mnist:\n",
        "        labels = labels.cuda()\n",
        "        total += labels.size(0)\n",
        "        inputs = inputs.reshape(test_batch_size, -1).cuda()\n",
        "        mu_z, logvar_z, y_pred = model.forward(inputs)\n",
        "        loss_test += nn.CrossEntropyLoss()(y_pred, labels.cuda())\n",
        "        y_pred = y_pred.max(1)[1]\n",
        "        correct += torch.eq(y_pred, labels).float().sum()\n",
        "    template = 'Epoch : {} || Loss : {:.3f} || Test --- Acc : {:.3f} || Loss : {:.3f}'\n",
        "    print(template.format(epoch + 1, loss, correct.cpu().numpy()/total, loss_test))\n",
        "\n",
        "  if epoch % decay_rate == 1:\n",
        "    optimizer.param_groups[0]['lr'] *= lr_decay\n",
        "model_b1 = model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 || Loss : 1.055 || Test --- Acc : 0.856 || Loss : 7.560\n",
            "Epoch : 2 || Loss : 0.876 || Test --- Acc : 0.869 || Loss : 6.178\n",
            "Epoch : 3 || Loss : 0.783 || Test --- Acc : 0.872 || Loss : 5.640\n",
            "Epoch : 4 || Loss : 0.727 || Test --- Acc : 0.873 || Loss : 5.276\n",
            "Epoch : 5 || Loss : 0.668 || Test --- Acc : 0.875 || Loss : 4.922\n",
            "Epoch : 6 || Loss : 0.634 || Test --- Acc : 0.877 || Loss : 4.737\n",
            "Epoch : 7 || Loss : 0.606 || Test --- Acc : 0.879 || Loss : 4.640\n",
            "Epoch : 8 || Loss : 0.580 || Test --- Acc : 0.879 || Loss : 4.472\n",
            "Epoch : 9 || Loss : 0.562 || Test --- Acc : 0.878 || Loss : 4.289\n",
            "Epoch : 10 || Loss : 0.542 || Test --- Acc : 0.882 || Loss : 4.246\n",
            "Epoch : 11 || Loss : 0.522 || Test --- Acc : 0.880 || Loss : 4.213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg2aiPQcZ7IK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "b0fdd0b0-133f-4bda-f397-d9dbf6b4e5ed"
      },
      "source": [
        "model = DeepIB(zDim = zDim, sampling = L).cuda()\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr0, (beta1, beta2))\n",
        "for epoch in range(nEpochs+1):\n",
        "  for i, (x_b, y_b) in enumerate(train_loader_mnist):\n",
        "    x_b = x_b.reshape(train_batch_size, -1).cuda()\n",
        "\n",
        "    optimizer.zero_grad()           ## No olvidar resetear los gradientes\n",
        "    mu_z, logvar_z, y_pred = model.forward(x_b)\n",
        "    loss = -1 *  fVAELoss(y_b, y_pred, mu_z, logvar_z, beta=0.05)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0 or epoch < 10:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_test = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader_mnist:\n",
        "        labels = labels.cuda()\n",
        "        total += labels.size(0)\n",
        "        inputs = inputs.reshape(test_batch_size, -1).cuda()\n",
        "        mu_z, logvar_z, y_pred = model.forward(inputs)\n",
        "        loss_test += nn.CrossEntropyLoss()(y_pred, labels.cuda())\n",
        "        y_pred = y_pred.max(1)[1]\n",
        "        correct += torch.eq(y_pred, labels).float().sum()\n",
        "    template = 'Epoch : {} || Loss : {:.3f} || Test --- Acc : {:.3f} || Loss : {:.3f}'\n",
        "    print(template.format(epoch + 1, loss, correct.cpu().numpy()/total, loss_test))\n",
        "\n",
        "  if epoch % decay_rate == 1:\n",
        "    optimizer.param_groups[0]['lr'] *= lr_decay\n",
        "model_b2 = model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 || Loss : 2.290 || Test --- Acc : 0.133 || Loss : 22.798\n",
            "Epoch : 2 || Loss : 2.284 || Test --- Acc : 0.158 || Loss : 22.632\n",
            "Epoch : 3 || Loss : 2.253 || Test --- Acc : 0.160 || Loss : 22.529\n",
            "Epoch : 4 || Loss : 2.304 || Test --- Acc : 0.171 || Loss : 22.335\n",
            "Epoch : 5 || Loss : 2.280 || Test --- Acc : 0.186 || Loss : 22.095\n",
            "Epoch : 6 || Loss : 2.251 || Test --- Acc : 0.198 || Loss : 21.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXxALehUZ7nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DeepIB(zDim = zDim, sampling = L).cuda()\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr0, (beta1, beta2))\n",
        "for epoch in range(nEpochs+1):\n",
        "  for i, (x_b, y_b) in enumerate(train_loader_mnist):\n",
        "    x_b = x_b.reshape(train_batch_size, -1).cuda()\n",
        "\n",
        "    optimizer.zero_grad()           ## No olvidar resetear los gradientes\n",
        "    mu_z, logvar_z, y_pred = model.forward(x_b)\n",
        "    loss = -1 *  fVAELoss(y_b, y_pred, mu_z, logvar_z, beta=1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0 or epoch < 10:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_test = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader_mnist:\n",
        "        labels = labels.cuda()\n",
        "        total += labels.size(0)\n",
        "        inputs = inputs.reshape(test_batch_size, -1).cuda()\n",
        "        mu_z, logvar_z, y_pred = model.forward(inputs)\n",
        "        loss_test += nn.CrossEntropyLoss()(y_pred, labels.cuda())\n",
        "        y_pred = y_pred.max(1)[1]\n",
        "        correct += torch.eq(y_pred, labels).float().sum()\n",
        "    template = 'Epoch : {} || Loss : {:.3f} || Test --- Acc : {:.3f} || Loss : {:.3f}'\n",
        "    print(template.format(epoch + 1, loss, correct.cpu().numpy()/total, loss_test))\n",
        "\n",
        "  if epoch % decay_rate == 1:\n",
        "    optimizer.param_groups[0]['lr'] *= lr_decay\n",
        "model_b3 = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHToYEtjZ-lU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MLP().cuda()\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr0, (beta1, beta2))\n",
        "for epoch in range(nEpochs+1):\n",
        "  for i, (x_b, y_b) in enumerate(train_loader_mnist):\n",
        "    x_b = x_b.reshape(train_batch_size, -1).cuda()\n",
        "\n",
        "    optimizer.zero_grad()           ## No olvidar resetear los gradientes\n",
        "    y_pred = model.forward(x_b)\n",
        "    loss = nn.CrossEntropyLoss()(y_pred, y_b.cuda())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0 or epoch < 10:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_test = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader_mnist:\n",
        "        labels = labels.cuda()\n",
        "        total += labels.size(0)\n",
        "        inputs = inputs.reshape(test_batch_size, -1).cuda()\n",
        "        y_pred = model.forward(inputs)\n",
        "        loss_test += nn.CrossEntropyLoss()(y_pred, labels.cuda())\n",
        "        y_pred = y_pred.max(1)[1]\n",
        "        correct += torch.eq(y_pred, labels).float().sum()\n",
        "    template = 'Epoch : {} || Loss : {:.3f} || Test --- Acc : {:.3f} || Loss : {:.3f}'\n",
        "    print(template.format(epoch + 1, loss, correct.cpu().numpy()/total, loss_test))\n",
        "\n",
        "  if epoch % decay_rate == 1:\n",
        "    optimizer.param_groups[0]['lr'] *= lr_decay\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwcUAHeKWRPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graficos\n",
        "data_loader = torch.utils.data.DataLoader(test_dataset_minist, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yh5mZ9FYcrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for inputs, labels in data_loader:\n",
        "  inputs = inputs.reshape(labels.shape[0], -1).cuda()\n",
        "  mu_z, sigma_z = model_b1.encode(inputs)\n",
        "  z_1 = model_b1.reparametrize(mu_z, sigma_z).cpu().detach().numpy().reshape(-1,2)\n",
        "  mu_z, sigma_z = model_b2.encode(inputs)\n",
        "  z_2 = model_b2.reparametrize(mu_z, sigma_z).cpu().detach().numpy().reshape(-1,2)\n",
        "  mu_z, sigma_z = model_b3.encode(inputs)\n",
        "  z_3 = model_b3.reparametrize(mu_z, sigma_z).cpu().detach().numpy().reshape(-1,2)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}