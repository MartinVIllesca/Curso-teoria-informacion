{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea_Capsula_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinVIllesca/Curso-teoria-informacion/blob/master/Tarea_Capsula_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyaqPYVJe9jC",
        "colab_type": "text"
      },
      "source": [
        "# Tarea Cápsula 1\n",
        "#### EL7014 Teorí­a de la Información: Fundamentos y Aplicaciones <br> Information and Decision Systems Group\n",
        "\n",
        "Nombre : Martín Valderrama Illesca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w24D_Wkfgi4",
        "colab_type": "text"
      },
      "source": [
        "## 1. Sección Teórica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgBlH79xn-i",
        "colab_type": "text"
      },
      "source": [
        "### 1.0. Teoremas y definiciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXO1bVKQ4OiF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Se adjuntan algunos teoremas y definiciones que le serán útiles para el desarrollo de la sección teórica.\n",
        "\n",
        "**Teorema de aproximación universal de las redes neuronales**\n",
        "\n",
        "Para efectos prácticos considere que dado un espacio paramérico $\\Theta$ compacto y un espacio $\\Omega$ también compacto y una función $T: \\Omega -> \\mathbb{R}$ se cumple que $\\forall \\epsilon > 0$  existe una red neuronal parametrizada $T_\\theta$ con parámetros $\\theta \\in \\Theta$ tal que.\n",
        "\n",
        "\\begin{equation}\n",
        "||T - T_{\\theta}|| < \\epsilon\n",
        "\\end{equation}\n",
        "\n",
        "**Ley uniforme de los grandes Números**\n",
        "\n",
        "Sea  $\\Theta$ compacto y $T(x,\\theta)$ una función medible la cual $\\forall \\theta$ es acotada por otra funcion medible $g(x)$ con posee esperanza finita sobre $p_X$. Entonces el estimador empírico de la esperanza dado una secuencia de muestras $(x_i)_{i=1}^N$ *i.i.d.* con distribucion $p_X$ cumple lo siguiente.\n",
        "\n",
        "\\begin{equation}\n",
        "\\sup_{\\theta \\in \\Theta}||\\mathbb{E}_{X^{(n)}}[T(X,\\theta)] - \\mathbb{E}_X[T(X,\\theta)]|| \\xrightarrow{c.s.} 0\n",
        "\\end{equation}\n",
        "\n",
        "**Lipschitz continuo con parámetro K**\n",
        "\n",
        "Se dice que una función $f$ es Lipschitz continua con parámetro $K$ en el intervalo $[a,b]$ si $\\forall x,y \\in [a,b]$ se cumple que.\n",
        "\n",
        "\\begin{equation}\n",
        "||f(y)-f(x)|| \\leq K ||y-x||\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOE-dKtQmWtz",
        "colab_type": "text"
      },
      "source": [
        "### 1.a. Representación Donsker-Varadhan "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUN0v0GEdii3",
        "colab_type": "text"
      },
      "source": [
        "Sea $T :\\Omega \\rightarrow \\mathbb{R}$ una función medible que cumple los criterios de integrabilidad para la esperanza y $X$ e $Y$ v.a. en $\\Omega$. \n",
        "\n",
        "\n",
        "a.1) Demuestre que $D_{KL}(p_x||p_y) \\geq \\mathbb{E}_X[T(\\omega)] - \\log \\mathbb{E}_Y[e^{T(\\omega)}]$\n",
        "\n",
        "**Indicacion:** Comience planteando $\\mathbb{E}_X[T(\\omega)] = \\mathbb{E}_X[\\frac{p_X}{p_Y}\\frac{p_Y}{p_X}T(\\omega)]$ y exprese la integral de la esperanza. Puede que más adelante le sirva utilizar alguna desigualdad vista en el curso.\n",
        "\n",
        "a.2) Muestre para que funcion $T^*$ se alcanza la igualdad de a.1)\n",
        "\n",
        "**Indicacion:** Escoja una funcion de tal manera que uno de los 2 terminos de la derecha se torne 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePRGnXp6md8z",
        "colab_type": "text"
      },
      "source": [
        "### 1.b. Consistencia del estimador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LUHsdcJmSwz",
        "colab_type": "text"
      },
      "source": [
        "Recordando la difinicion de los siguientes valores.\n",
        "\n",
        "\\begin{equation}\n",
        "I_{\\Theta}(X;Y) = \\sup_{\\theta \\in \\Theta} \\mathbb{E}_{X,Y}[T_\\theta(\\omega)] - \\log \\mathbb{E}_{X\\bigotimes Y}[e^{T_\\theta(\\omega)}]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{I}(X;Y)_{n} = \\sup_{\\theta \\in \\Theta} \\mathbb{E}_{X,Y^{(n)}}[T_\\theta(\\omega)] - \\log \\mathbb{E}_{X^{(n)}\\bigotimes Y^{(n)}}[e^{T_\\theta(\\omega)}]\n",
        "\\end{equation}\n",
        "\n",
        "Sea $\\Omega \\times \\Theta$ un espacio compacto, es decir, toda red caracterizada como $T:\\Omega \\times \\Theta \\rightarrow \\mathbb{R}$ es acotada por una constante $K$ fija determinada por el dominio y $X$ e $Y$ v.a. en $\\Omega$.\n",
        "\n",
        "Notar que tanto $I_{\\Theta}(X;Y)$ como $\\hat{I}(X;Y)_{n}$ siempre estan acotados por una variable $K$ dado que $\\Omega$ es definido compacto.\n",
        "\n",
        "b.1) Demuestre que \\begin{equation}\n",
        "|I(X;Y)-I_\\Theta(X;Y)| \\leq \\epsilon \n",
        "\\end{equation}\n",
        "* Escriba $I(X;Y)$ utilizando la funcion $T^*$ de a.2) y note el valor que toma la esperanza sobre la multiplicación de las marginales $\\mathbb{E}_{X \\times Y}[e^{T^*}]$.\n",
        "* Utilice la cota $\\log a \\leq a-1$ e identifique $\\mathbb{E}_{X \\times Y}[e^{T^*}]$ como el valor numérico que toma y reemplacelo.\n",
        "* Utilice el hecho que la funcion $e$ es Lipschitz continua con parámetro $e^c$ en el intervalo $(-\\infty, c]$.\n",
        "* Utilice el teorema de aproximacion universal de las redes neuronales para acotar las esperanzas por una cota superior inteligentemente escogida.\n",
        "* Concluya utilizando una desigualdad triangular.\n",
        "\n",
        "b.2) Demuestre que \\begin{equation}\n",
        "\\hat{I}(X;Y)_n \\xrightarrow{c.s.} I_\\Theta(X;Y)\n",
        "\\end{equation}\n",
        "\n",
        "* Utilice el hecho que la funcion $\\log$ es Lipschitz continua con parámetro $e^c$ en el intervalo $[e^{-c}, e^c]$.\n",
        "* Luego utilice la Ley uniforme de los grandes números sobre las distribuciones empí­ricas para introducir el $\\lim_{n \\rightarrow \\infty}$.\n",
        "* Concluya utilizando una desigualdad triangular.\n",
        "\n",
        "b.3) Use b.1) y b.2) con una desigualdad triangular para demostrar que \\begin{equation}\n",
        "\\hat{I}(X;Y)_n \\xrightarrow{p}I(X;Y).\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcK2qbXTfoL8",
        "colab_type": "text"
      },
      "source": [
        "## 2. Sección Práctica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJ_rXq8wbiE",
        "colab_type": "text"
      },
      "source": [
        "Utilizando el sampler gaussiano dispuesto entrene 2 redes MINE que maximicen las funciones objetivo:\n",
        "\n",
        "* $\\mathbb{E}_{X,Y^{(n)}}[T_\\theta(\\omega)] - \\log \\mathbb{E}_{X^{(n)}\\bigotimes Y^{(n)}}[e^{T_\\theta(\\omega)}]$ \\\\\n",
        "\n",
        "* $\\mathbb{E}_{X,Y^{(n)}}[T_\\theta(\\omega)] - \\mathbb{E}_{X^{(n)}\\bigotimes Y^{(n)}}[e^{T_\\theta(\\omega)-1}]$\n",
        "\n",
        "Las pruebas deben ser utilizando los parámetros del sampler como:\n",
        "\n",
        "* dim = 4 y 10 valores equiespaciados de $\\rho$ entre $[-0.9,0.9]$\n",
        "* dim = 6 y 10 valores equiespaciados de $\\rho$ entre $[-0.9,0.9]$\n",
        "\n",
        "Presente los gráficos comparativos de $\\hat{I}(X;Y)$ y $I_{theoretical}(X,Y)$ vs $\\rho$ como se muestra en la cápsula 1. \n",
        "\n",
        "4 gráficos en total (ambas funciones objetivos para ambos casos de test).\n",
        "\n",
        "**Indicación:** **NO** considere el ajuste para insesgar el gradiente para el caso que utiliza la representacion D-V, i.e. ocupe directamente la función objetivo y deje que pytorch propage el gradiente automáticamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL0l3YUZwNRS",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0O6fSgydZWF",
        "colab_type": "code",
        "outputId": "508eac23-2ce8-4c78-e12e-40e63581c49f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHGeXgMreK6C",
        "colab_type": "code",
        "outputId": "5dbee6b3-6e19-43b9-8b5f-cee7a7c7ed67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Asegurarse que Colab les asignÃ³ un entorno con GPU si la desean usar para entrenamiento\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SJHi1VHwl55",
        "colab_type": "text"
      },
      "source": [
        "### Sampler Gaussiano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUyc_Fdzz1hu",
        "colab_type": "text"
      },
      "source": [
        "La función \"multivariate_gaussian_sampler\" genera muestras de 2 variables aleatorias sobre $\\mathbb{R}^d$ de medias vectores 0 y covarianza como se describe:\n",
        "\\begin{equation}\n",
        "Cov(\\bar{X},\\bar{Y}) = \\begin{bmatrix}\n",
        "I_{dxd} & \\rho I_{dxd}\\\\\n",
        "\\rho I_{dxd} & I_{dxd}\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdROGV7ReXqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multivariate_gaussian_sampler(dim, corr_factor, n_samples):\n",
        "  '''\n",
        "  Generates samples from two gaussian random variables X_a and X_b of \n",
        "  dimension dim with Corr(X_a^i, X_b^j) = delta_ij * corr_factor, returning the \n",
        "  theoretical mutual information MI(X_a, X_b).\n",
        "  '''\n",
        "  joint_mean = np.zeros(2 * dim)\n",
        "  identity = np.identity(dim)\n",
        "  joint_cov = np.concatenate([np.concatenate([identity, corr_factor * identity], axis=1),\n",
        "                              np.concatenate([corr_factor * identity, identity], axis=1)], \n",
        "                             axis=0)\n",
        "  np.fill_diagonal(joint_cov, 1)\n",
        "  theoretical_mi = -(1/2) * np.log(np.linalg.det(joint_cov))\n",
        "  X_joint = np.random.multivariate_normal(joint_mean, joint_cov, size=n_samples)\n",
        "  X_a = X_joint[:, :dim]\n",
        "  X_b = X_joint[:, dim:]\n",
        "  return X_joint, X_a, X_b, theoretical_mi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lM9ZA4hhINM",
        "colab_type": "text"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thj2TZxahHnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}